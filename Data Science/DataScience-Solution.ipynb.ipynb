{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5d80c60",
   "metadata": {
    "id": "f5d80c60"
   },
   "source": [
    "# **1. Business Understanding**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emeobxKHSDUU",
   "metadata": {
    "id": "emeobxKHSDUU"
   },
   "source": [
    "## 1. What is the business problem that you are trying to solve?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3298e121",
   "metadata": {
    "id": "3298e121"
   },
   "source": [
    "Music streaming platforms like **Spotify** and video-sharing platforms like **YouTube** play a crucial role in the popularity of songs. However, there is a disparity in how certain songs perform across these platforms. Some tracks may have **millions of streams on Spotify but relatively fewer views on YouTube, and vice versa**.\n",
    "\n",
    "The key questions we aim to answer:\n",
    "- **What factors contribute to a songâ€™s popularity on Spotify versus YouTube?**\n",
    "- **Can we predict whether a song will perform better on Spotify or YouTube based on its attributes?**\n",
    "- **What are the key characteristics that define a hit song across different platforms?**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KofnBTt0SM7c",
   "metadata": {
    "id": "KofnBTt0SM7c"
   },
   "source": [
    "## 2. What data do you need to answer the above problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "x0BZU3-kSKQO",
   "metadata": {
    "id": "x0BZU3-kSKQO"
   },
   "source": [
    "To answer these questions, we need data that includes:\n",
    "- **Song metadata:** Track name, artist, album, and release type.\n",
    "- **Spotify streaming data:** Number of streams, danceability, energy, loudness, speechiness, tempo, acousticness, instrumentalness, etc.\n",
    "- **YouTube performance metrics:** Views, likes, comments, and official video status.\n",
    "- **Musical characteristics:** Attributes like key, liveness, and valence to understand the songâ€™s emotional and structural appeal.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bl_hvtzESeJW",
   "metadata": {
    "id": "bl_hvtzESeJW"
   },
   "source": [
    "## 3. What are the different sources of data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "j96PIAlkSVZW",
   "metadata": {
    "id": "j96PIAlkSVZW"
   },
   "source": [
    "Dataset is taken from Kaggle and it combines data from **Spotify** and **YouTube**. The data is likely collected using:\n",
    "- **Spotify API**\n",
    "- **YouTube API**\n",
    "- **Web scraping or third-party repositories**\n",
    "\n",
    "****\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-aOGuxQUSaUN",
   "metadata": {
    "id": "-aOGuxQUSaUN"
   },
   "source": [
    "## 4. What kind of analytics task are you performing?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2Utstl_uSXZd",
   "metadata": {
    "id": "2Utstl_uSXZd"
   },
   "source": [
    "This project involves:\n",
    "- **Exploratory Data Analysis (EDA):** Understanding patterns and trends in song popularity across Spotify and YouTube.\n",
    "- **Feature Engineering:** Identifying key characteristics that impact song performance.\n",
    "- **Predictive Modeling:** Building a machine learning model to classify whether a song is more likely to be a **Spotify Hit** or a **YouTube Hit**.\n",
    "- **Clustering:** Grouping songs based on similar characteristics to understand common patterns in song popularity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZQmXpmjCOvFm",
   "metadata": {
    "id": "ZQmXpmjCOvFm"
   },
   "source": [
    "# **Importing Packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "H4skYXn0OuX5",
   "metadata": {
    "id": "H4skYXn0OuX5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import zscore\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "import numpy as np\n",
    "from scipy.stats import chi2_contingency\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.feature_selection import mutual_info_classif, chi2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc8e0cb",
   "metadata": {
    "id": "3cc8e0cb"
   },
   "source": [
    "# **2. Data Acquisition**\n",
    "\n",
    "For the problem identified, we will use the **Spotify and YouTube Dataset** that contains various statistics of songs from both platforms.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "## 2.1 Download the data directly\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "wyZ4uorOQmjb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wyZ4uorOQmjb",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "11848659-8766-4bb9-d61b-033bf2e67fc1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'kaggle' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset successfully downloaded!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'unzip' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# Download dataset from Kaggle\n",
    "!kaggle datasets download -d salvatorerastelli/spotify-and-youtube\n",
    "\n",
    "# Unzip the dataset\n",
    "!unzip -o spotify-and-youtube.zip\n",
    "\n",
    "print(\"Dataset successfully downloaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49530d0c",
   "metadata": {
    "id": "49530d0c"
   },
   "source": [
    "## 2.2 Code for converting the above downloaded data into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1f4c171",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "c1f4c171",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "d30b2c0c-594f-4196-b55e-daf7d60edcac"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Spotify_Youtube.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load the dataset\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpotify_Youtube.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset successfully loaded!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1881\u001b[0m     f,\n\u001b[0;32m   1882\u001b[0m     mode,\n\u001b[0;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1889\u001b[0m )\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Spotify_Youtube.csv'"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv(\"Spotify_Youtube.csv\")\n",
    "print(\"Dataset successfully loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1fea4d",
   "metadata": {
    "id": "7b1fea4d"
   },
   "source": [
    "## 2.3 Confirm the data has been correctly by displaying the first 5 and last 5 records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624e6c58",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "624e6c58",
    "outputId": "161350cc-f238-4bca-b8e1-4f62bf7133bf"
   },
   "outputs": [],
   "source": [
    "# Display the first 5 rows\n",
    "print(\"First 5 records:\")\n",
    "display(df.head())\n",
    "\n",
    "# Display the last 5 rows\n",
    "print(\"\\nLast 5 records:\")\n",
    "display(df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb84fc56",
   "metadata": {
    "id": "bb84fc56"
   },
   "source": [
    "## 2.4 Display the column headings, statistical information, description and statistical summary of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086ad28e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "086ad28e",
    "outputId": "b1fa3215-e35e-40e5-a1cf-9df3d4c339bc"
   },
   "outputs": [],
   "source": [
    "# Display column names\n",
    "print(\"Column Names:\")\n",
    "print(df.columns)\n",
    "\n",
    "# Show basic information about dataset (data types, missing values, etc.)\n",
    "print(\"\\nDataset Info:\")\n",
    "df.info()\n",
    "\n",
    "# Display statistical summary for numerical columns\n",
    "print(\"\\nStatistical Summary:\")\n",
    "display(df.describe())\n",
    "\n",
    "# Display summary for categorical columns\n",
    "print(\"\\nCategorical Data Summary:\")\n",
    "display(df.describe(include=['object']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812edb18",
   "metadata": {
    "id": "812edb18"
   },
   "source": [
    "# 2.5 Observations from the Dataset\n",
    "#### **1. Size of the Dataset**\n",
    "- The dataset contains **`20718` rows** and **`28` columns**.\n",
    "\n",
    "#### **2. Types of Data Attributes**\n",
    "\n",
    "**Data types of attributes:**\n",
    "\n",
    "float64 -   15,\n",
    "object  -   12,\n",
    "int64    -   1\n",
    "\n",
    "Name: count, dtype: int64\n",
    "\n",
    "The dataset consists of **numerical** and **categorical** attributes.\n",
    "\n",
    "**Numerical Attributes (Continuous & Discrete)**  \n",
    "These represent measurable quantities:\n",
    "- `streams` â†’ Number of streams on Spotify  \n",
    "- `views` â†’ Number of views on YouTube  \n",
    "- `likes` â†’ Number of likes on YouTube  \n",
    "- `comments` â†’ Number of comments on YouTube  \n",
    "- `danceability` â†’ Suitability for dancing (0 to 1)  \n",
    "- `energy` â†’ Measure of intensity (0 to 1)  \n",
    "- `loudness` â†’ Overall loudness (in dB)  \n",
    "- `speechiness` â†’ Presence of spoken words (0 to 1)  \n",
    "- `acousticness` â†’ Likelihood of being acoustic (0 to 1)  \n",
    "- `instrumentalness` â†’ Likelihood of being instrumental (0 to 1)  \n",
    "- `liveness` â†’ Probability of a live performance (0 to 1)  \n",
    "- `valence` â†’ Musical positivity (0 to 1)  \n",
    "- `tempo` â†’ Tempo of the track (beats per minute)  \n",
    "- `duration_ms` â†’ Track duration (in milliseconds)  \n",
    "\n",
    "**Categorical Attributes (Nominal & Ordinal)**  \n",
    "These represent categories or labels:\n",
    "- `track` â†’ Name of the song  \n",
    "- `artist` â†’ Name of the artist  \n",
    "- `album` â†’ Name of the album  \n",
    "- `album_type` â†’ Single or Album  \n",
    "- `channel` â†’ Name of the YouTube channel  \n",
    "- `official_video` â†’ Boolean (True/False) indicating if it's the official video  \n",
    "- `url_spotify` â†’ Spotify URL of the track  \n",
    "- `url_youtube` â†’ YouTube URL of the video  \n",
    "- `description` â†’ Description of the YouTube video  \n",
    "- `licensed` â†’ Boolean indicating if the video is officially licensed  \n",
    "\n",
    "---\n",
    "\n",
    "#### **3. Missing Data Check**\n",
    "\n",
    "The table below shows the number of missing values for each column:\n",
    "\n",
    "| **Feature**            | **Missing Values** |\n",
    "|------------------------|-------------------|\n",
    "| Danceability          | 2   |\n",
    "| Energy                | 2   |\n",
    "| Key                   | 2   |\n",
    "| Loudness              | 2   |\n",
    "| Speechiness           | 2   |\n",
    "| Acousticness          | 2   |\n",
    "| Instrumentalness      | 2   |\n",
    "| Liveness              | 2   |\n",
    "| Valence               | 2   |\n",
    "| Tempo                 | 2   |\n",
    "| Duration_ms           | 2   |\n",
    "| Url_youtube           | 470 |\n",
    "| Title                 | 470 |\n",
    "| Channel               | 470 |\n",
    "| Views                 | 470 |\n",
    "| Likes                 | 541 |\n",
    "| Comments              | 569 |\n",
    "| Description           | 876 |\n",
    "| Licensed              | 470 |\n",
    "| Official_video        | 470 |\n",
    "| Stream               | 576 |\n",
    "\n",
    "#### **ðŸ”¹ Percentage of Missing Values**\n",
    "The following table displays the percentage of missing values for each column:\n",
    "\n",
    "| **Feature**            | **Missing %** |\n",
    "|------------------------|--------------|\n",
    "| Danceability          | 0.0097%  |\n",
    "| Energy                | 0.0097%  |\n",
    "| Key                   | 0.0097%  |\n",
    "| Loudness              | 0.0097%  |\n",
    "| Speechiness           | 0.0097%  |\n",
    "| Acousticness          | 0.0097%  |\n",
    "| Instrumentalness      | 0.0097%  |\n",
    "| Liveness              | 0.0097%  |\n",
    "| Valence               | 0.0097%  |\n",
    "| Tempo                 | 0.0097%  |\n",
    "| Duration_ms           | 0.0097%  |\n",
    "| Url_youtube           | 2.27%  |\n",
    "| Title                 | 2.27%  |\n",
    "| Channel               | 2.27%  |\n",
    "| Views                 | 2.27%  |\n",
    "| Likes                 | 2.61%  |\n",
    "| Comments              | 2.75%  |\n",
    "| Description           | 4.23%  |\n",
    "| Licensed              | 2.27%  |\n",
    "| Official_video        | 2.27%  |\n",
    "| Stream               | 2.78%  |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "v5irSksmb0b_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v5irSksmb0b_",
    "outputId": "f67be127-b316-457f-850f-8445b4e7716d"
   },
   "outputs": [],
   "source": [
    "# 1. Size of the dataset\n",
    "num_rows, num_cols = df.shape\n",
    "print(f\"Dataset contains {num_rows} rows and {num_cols} columns.\\n\")\n",
    "\n",
    "# 2. Data types of attributes\n",
    "print(\"Data types of attributes:\")\n",
    "print(df.dtypes.value_counts(), \"\\n\")  # Count of different data types\n",
    "\n",
    "# 3. Checking for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "missing_values = missing_values[missing_values > 0]  # Only show columns with missing values\n",
    "\n",
    "if missing_values.empty:\n",
    "    print(\"No missing values in the dataset.\")\n",
    "else:\n",
    "    print(\"Missing values in the dataset:\")\n",
    "    print(missing_values)\n",
    "\n",
    "    # Percentage of missing values\n",
    "    missing_percentage = (missing_values / len(df)) * 100\n",
    "    print(\"\\nPercentage of missing values:\")\n",
    "    print(missing_percentage)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102e0e36",
   "metadata": {
    "id": "102e0e36"
   },
   "source": [
    "# 3. Data Preparation"
   ]
  },
  {
   "cell_type": "raw",
   "id": "637cb469",
   "metadata": {
    "id": "637cb469"
   },
   "source": [
    "If input data is numerical or categorical, do 3.1, 3.2 and 3.4\n",
    "If input data is text, do 3.3 and 3.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcb953b",
   "metadata": {
    "id": "2bcb953b"
   },
   "source": [
    "## 3.1 Check for\n",
    "\n",
    "* duplicate data\n",
    "* missing data\n",
    "* data inconsistencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5b960e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5a5b960e",
    "outputId": "8b3aad95-9c17-431d-9417-2e92f2d8c09a"
   },
   "outputs": [],
   "source": [
    "# Check for duplicate rows\n",
    "duplicate_count = df.duplicated().sum()\n",
    "print(f\"Total Duplicate Rows: {duplicate_count}\")\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "missing_percentage = (df.isnull().sum() / len(df)) * 100\n",
    "print(f\"Missing values:, {missing_values}\")\n",
    "print(f\"Missing percentage:, {missing_percentage}\")\n",
    "\n",
    "print('\\n Check for inconsistent data')\n",
    "# Identify columns where negative values should not exist\n",
    "num_cols_to_check = ['Views', 'Likes', 'Comments', 'Duration_ms', 'Stream']\n",
    "\n",
    "# Filter rows with negative values\n",
    "inconsistent_rows = df[(df[num_cols_to_check] < 0).any(axis=1)]\n",
    "\n",
    "# Display inconsistent rows\n",
    "print(f\"Rows with Negative Values:\\n{inconsistent_rows}\")\n",
    "\n",
    "# Checking unique values in categorical columns\n",
    "cat_cols_to_check = ['official_video', 'Licensed']\n",
    "\n",
    "for col in cat_cols_to_check:\n",
    "    print(f\"Unique values in {col}: {df[col].unique()}\")\n",
    "\n",
    "# Compute Z-scores for numerical columns\n",
    "z_scores = df[num_cols_to_check].apply(zscore)\n",
    "\n",
    "# Define a threshold (e.g., |z| > 3 indicates a potential outlier)\n",
    "outliers = df[(z_scores > 3).any(axis=1)]\n",
    "\n",
    "print(f\"Potential Outliers (Z-score > 3):\\n{outliers}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fdebf8",
   "metadata": {
    "id": "06fdebf8"
   },
   "source": [
    "## 3.2 Apply techiniques\n",
    "* to remove duplicate data\n",
    "* to impute or remove missing data\n",
    "* to remove data inconsistencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3118eb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dd3118eb",
    "outputId": "967a1b48-d50d-4e79-b9f1-f19f0b55ce19"
   },
   "outputs": [],
   "source": [
    "# Remove duplicate rows\n",
    "df = df.drop_duplicates()\n",
    "print(\"Duplicates removed successfully!\")\n",
    "\n",
    "# Calculate missing values percentage\n",
    "missing_percentage = df.isnull().sum() / len(df) * 100\n",
    "print(\"Missing Data Percentage:\\n\", missing_percentage)\n",
    "\n",
    "# Define threshold: If more than 30% of data is missing, drop the column\n",
    "missing_threshold = 0.3 * len(df)\n",
    "df = df.dropna(thresh=missing_threshold, axis=1)\n",
    "\n",
    "# Impute missing numerical data with median\n",
    "num_cols = df.select_dtypes(include=['number']).columns\n",
    "df[num_cols] = df[num_cols].fillna(df[num_cols].median())\n",
    "\n",
    "# Impute missing categorical data with mode\n",
    "cat_cols = df.select_dtypes(include=['object']).columns\n",
    "df[cat_cols] = df[cat_cols].fillna(df[cat_cols].mode().iloc[0])\n",
    "\n",
    "print(\"Missing data handled successfully!\")\n",
    "\n",
    "# Replace negative values with NaN (for later imputation)\n",
    "num_cols_to_check = ['Views', 'Likes', 'Comments', 'Duration_ms', 'Stream']\n",
    "df[num_cols_to_check] = df[num_cols_to_check].apply(lambda x: x.mask(x < 0, None))\n",
    "\n",
    "# Impute the newly created NaNs with the median\n",
    "df[num_cols_to_check] = df[num_cols_to_check].fillna(df[num_cols_to_check].median())\n",
    "\n",
    "print(\"Negative values corrected!\")\n",
    "\n",
    "# Convert categorical values to lowercase\n",
    "df['official_video'] = df['official_video'].astype(str).str.lower()\n",
    "df['Licensed'] = df['Licensed'].astype(str).str.lower()\n",
    "\n",
    "print(\"Categorical inconsistencies fixed!\")\n",
    "\n",
    "print(\"Final dataset shape:\", df.shape)\n",
    "print(\"Final missing values:\\n\", df.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2139fedf",
   "metadata": {
    "id": "2139fedf"
   },
   "source": [
    "## 3.3 Encode categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6886fc",
   "metadata": {
    "id": "da6886fc"
   },
   "outputs": [],
   "source": [
    "# Only for text data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cec4fc",
   "metadata": {
    "id": "e3cec4fc"
   },
   "source": [
    "## 3.4 Report\n",
    "\n",
    "Mention and justify the method adopted\n",
    "* to remove duplicate data, if present\n",
    "* to impute or remove missing data, if present\n",
    "* to remove data inconsistencies, if present\n",
    "\n",
    "OR for textdata\n",
    "* How many tokens after step 3?\n",
    "* how may tokens after stop words filtering?\n",
    "\n",
    "If the any of the above are not present, then also add in the report below.\n",
    "\n",
    "Score: 2 Marks (based on the dataset you have, the data prepreation you had to do and report typed, marks will be distributed between 3.1, 3.2, 3.3 and 3.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CAIfAfEmadue",
   "metadata": {
    "id": "CAIfAfEmadue"
   },
   "source": [
    "#### **1. Handling Duplicate Data**  \n",
    "- **Method Used:** Removed duplicate rows using `df.drop_duplicates()`.  \n",
    "- **Justification:**  \n",
    "  - Prevents redundant data that could skew analysis.  \n",
    "  - Ensures unique and accurate data representation.  \n",
    "\n",
    "<br>\n",
    "\n",
    "#### **2. Handling Missing Data**  \n",
    "- **Method Used:**  \n",
    "  - Dropped columns with **>30% missing values** (`df.dropna(thresh=missing_threshold, axis=1)`).  \n",
    "  - Imputed **numerical values with the median** (`df[num_cols].fillna(df[num_cols].median())`).  \n",
    "  - Imputed **categorical values with the mode** (`df[cat_cols].fillna(df[cat_cols].mode().iloc[0])`).  \n",
    "\n",
    "- **Justification:**  \n",
    "  - Retains most of the data while eliminating excessive missing values.  \n",
    "  - Median imputation prevents skewing by outliers.  \n",
    "  - Mode imputation maintains categorical data consistency.  \n",
    "\n",
    "<br>\n",
    "\n",
    "#### **3. Handling Data Inconsistencies**  \n",
    "- **Method Used:**  \n",
    "  - Replaced **negative values** in numerical columns with `NaN`, then imputed them with the median.  \n",
    "  - Standardized categorical variables by converting them to **lowercase**.  \n",
    "\n",
    "- **Justification:**  \n",
    "  - Negative values in attributes like **views, likes, comments, and duration** are logically incorrect.  \n",
    "  - Standardizing text data ensures uniform representation for analysis.  \n",
    "\n",
    "<br>\n",
    "\n",
    "### **Final Observations**  \n",
    "**Duplicates removed** â€“ Ensures a clean dataset.  \n",
    "**Missing values handled effectively** â€“ Prevents loss of significant data.  \n",
    "**Data inconsistencies corrected** â€“ Enhances data integrity for further analysis.  \n",
    "**Final dataset is cleaned and ready for exploratory data analysis (EDA) and modeling.**  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793cd04b",
   "metadata": {
    "id": "793cd04b"
   },
   "source": [
    "## 3.5 Identify the target variables.\n",
    "\n",
    "* Separate the data from the target such that the dataset is in the form of (X,y) or (Features, Label)\n",
    "\n",
    "* Discretize / Encode the target variable or perform one-hot encoding on the target or any other as and if required.\n",
    "\n",
    "* Report the observations\n",
    "\n",
    "Score: 1 Mark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9089b57",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c9089b57",
    "outputId": "5a8501e1-f4c1-414a-8e77-9c16116173a2"
   },
   "outputs": [],
   "source": [
    "# Identify the Target Variable\n",
    "y = df['Views']\n",
    "\n",
    "# Separate Features (X) from Target (y)\n",
    "X = df.drop(columns=['Views'])\n",
    "print(f\".Features Shape: {X.shape}, Target Shape: {y.shape}\")\n",
    "\n",
    "# Identify categorical columns\n",
    "categorical_cols = ['Key', 'Title', 'Channel', 'Licensed', 'official_video']\n",
    "\n",
    "# Apply OneHotEncoding\n",
    "X = pd.get_dummies(X, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "print(f\"Shape after encoding: {X.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "joxfKg8s4A5-",
   "metadata": {
    "id": "joxfKg8s4A5-"
   },
   "source": [
    "### **Observations:**\n",
    "1. **Target Variable Selection:**  \n",
    "   - The target variable chosen is **`Views`**, as we are likely trying to predict the number of views based on song attributes.\n",
    "\n",
    "2. **Feature Separation:**  \n",
    "   - The dataset is now split into **features (`X`)** and **target (`y`)**.  \n",
    "   - Shape of `X`: `(num_samples, num_features)`  \n",
    "   - Shape of `y`: `(num_samples,)`  \n",
    "\n",
    "3. **Encoding Categorical Variables:**  \n",
    "   - Columns like **`Key`**, **`Title`**, **`Channel`**, **`Licensed`**, and **`official_video`** were categorical.  \n",
    "   - **One-Hot Encoding** was applied to convert them into numerical format.  \n",
    "   - This increased the number of features in `X`.\n",
    "\n",
    "4. **Impact of Encoding:**  \n",
    "   - Some categorical columns may have a large number of unique values (e.g., `Title`, `Channel`), which may lead to **high dimensionality**.  \n",
    "   - If necessary, we can **drop `Title` or `Channel`** if they do not contribute significantly to the model.\n",
    "\n",
    "5. **Next Steps:**  \n",
    "   - **Feature Scaling** may be required for numerical attributes.  \n",
    "   - **Feature Selection** can help reduce dimensionality and improve model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae0b5d2",
   "metadata": {
    "id": "3ae0b5d2"
   },
   "source": [
    "# 4. Data Exploration using various plots\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186bf4d7",
   "metadata": {
    "id": "186bf4d7"
   },
   "source": [
    "## 4.1 Scatter plot of each quantitative attribute with the target.\n",
    "\n",
    "Score: 1 Mark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868d7b27",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "868d7b27",
    "outputId": "fc0d0c76-4c66-4d8c-cb0c-1aae31672df5"
   },
   "outputs": [],
   "source": [
    "# Define numerical features\n",
    "numerical_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "# Set up the plotting grid\n",
    "plt.figure(figsize=(15, 20))\n",
    "for i, feature in enumerate(numerical_features, 1):\n",
    "    plt.subplot(5, 3, i)\n",
    "    sns.scatterplot(x=X[feature], y=y, alpha=0.5)\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel(\"Views\")\n",
    "    plt.title(f\"{feature} vs Views\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575f9e37",
   "metadata": {
    "id": "575f9e37"
   },
   "source": [
    "## 4.2 EDA using visuals\n",
    "* Use (minimum) 2 plots (pair plot, heat map, correlation plot, regression plot...) to identify the optimal set of attributes that can be used for classification.\n",
    "* Name them, explain why you think they can be helpful in the task and perform the plot as well. Unless proper justification for the choice of plots given, no credit will be awarded.\n",
    "\n",
    "Score: 2 Marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d614311",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "4d614311",
    "outputId": "93bbbd76-0727-44d9-9c5a-343693ecce2b"
   },
   "outputs": [],
   "source": [
    "# Select a subset of numerical features and the target variable\n",
    "selected_features = ['Danceability', 'Energy', 'Loudness', 'Speechiness', 'Instrumentalness', 'Valence', 'Tempo', 'Views']\n",
    "\n",
    "# Pair Plot\n",
    "sns.pairplot(df[selected_features])\n",
    "plt.suptitle(\"Pair Plot of Selected Attributes\", y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# Heatmap for Correlation Analysis\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(df[selected_features].corr(), annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Correlation Heatmap of Features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VK1m2CnwdwNL",
   "metadata": {
    "id": "VK1m2CnwdwNL"
   },
   "source": [
    "### **Why We Used Pair Plots and Correlation Plot?**  \n",
    "\n",
    "#### **1. Pair Plot:**  \n",
    "- A pair plot visualizes relationships between multiple numerical variables using scatterplots.\n",
    "- It helps in understanding **distributions, trends, and separability** of features, which are crucial for classification.  \n",
    "- It highlights **linear and nonlinear relationships** between features and target classes.  \n",
    "\n",
    "#### **2. Correlation Plot:**  \n",
    "- A correlation heatmap shows how strongly each feature is related to others using a **color-coded matrix**.  \n",
    "- It helps detect **redundant features** that provide similar information (e.g., Energy and Loudness).  \n",
    "- It allows us to **select features that contribute unique information** to classification.  \n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "### **How These Plots Help Identify Optimal Attributes for Classification?**  \n",
    "\n",
    "- The **Pair Plot** allows us to visually assess which attributes exhibit **distinct clusters or trends**, making them good candidates for classification.  \n",
    "- The **Correlation Heatmap** ensures that we **avoid multicollinearity** by removing redundant features while selecting the most relevant ones.  \n",
    "- Features that have **strong relationships with the target variable** (or distinguishable distributions) are preferred.  \n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "### **Observations from the output (refer above result)**  \n",
    "\n",
    "#### **Pair Plot Observations:**  \n",
    "1. **Energy vs. Loudness:**  \n",
    "   - There is a **strong positive relationship** between Energy and Loudness.  \n",
    "   - Since they provide similar information, we may use only one.  \n",
    "\n",
    "2. **Danceability vs. Valence:**  \n",
    "   - These two features show **some separability** in scatter plots.  \n",
    "   - They can be useful for classification, especially for song mood prediction.  \n",
    "\n",
    "3. **Speechiness and Instrumentalness:**  \n",
    "   - These variables show **random scatter** with no clear pattern.  \n",
    "   - They may not contribute significantly to classification.  \n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "#### **Correlation Heatmap Observations:**  \n",
    "1. **High Correlation:**  \n",
    "   - **Energy and Loudness (0.74)** â†’ Strong correlation, so we should use only one.  \n",
    "   - **Danceability and Valence (0.47)** â†’ Moderate correlation, meaning both can be useful.  \n",
    "\n",
    "2. **Low Correlation (Weak Predictors):**  \n",
    "   - **Speechiness and Instrumentalness** have low correlation with all other features, meaning they may not be strong predictors.  \n",
    "<br>\n",
    "<br>\n",
    "\n",
    "### **Conclusion:**  \n",
    "Based on these observations, the **best features** for classification are:  \n",
    "**Energy** (or Loudness, but not both)  \n",
    "**Danceability**  \n",
    "**Valence**  \n",
    "**Views** (useful for popularity-based classification)  \n",
    "**Tempo** (based on some weak but potential influence)  \n",
    "\n",
    "**Speechiness and Instrumentalness do not show strong relationships and may not be helpful for classification.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbc82a1",
   "metadata": {
    "id": "bdbc82a1"
   },
   "source": [
    "# 5. Data Wrangling\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca214eb3",
   "metadata": {
    "id": "ca214eb3"
   },
   "source": [
    "## 5.1 Univariate Filters\n",
    "\n",
    "#### Numerical and Categorical Data\n",
    "* Identify top 5 significant features by evaluating each feature independently with respect to the target/other variable by exploring\n",
    "1. Mutual Information (Information Gain)\n",
    "2. Gini index\n",
    "3. Gain Ratio\n",
    "4. Chi-Squared test\n",
    "5. Strenth of Association\n",
    "\n",
    "(From the above 5 you are required to use only any <b>two</b>)\n",
    "\n",
    "\n",
    "\n",
    "Score: 3 Marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85e9754",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a85e9754",
    "outputId": "08559149-9f74-423a-ef98-1649e23077e8"
   },
   "outputs": [],
   "source": [
    "# Define features and target\n",
    "numerical_features = ['Danceability', 'Energy', 'Loudness', 'Speechiness',\n",
    "                      'Instrumentalness', 'Valence', 'Tempo', 'Duration_ms', 'Likes', 'Comments']\n",
    "target = 'Views'\n",
    "\n",
    "X = df[numerical_features]\n",
    "y = df[target]\n",
    "\n",
    "# ðŸ”¹ Use mutual_info_regression() for continuous target\n",
    "mi_scores = mutual_info_regression(X, y)\n",
    "mi_scores = pd.Series(mi_scores, index=numerical_features).sort_values(ascending=False)\n",
    "\n",
    "print(\"Top 5 features based on Mutual Information:\\n\", mi_scores.head(5))\n",
    "\n",
    "# ðŸ”¹ Normalize features for Chi-Squared Test\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# ðŸ”¹ Chi-Squared Test\n",
    "chi_scores, p_values = chi2(X_scaled, y)\n",
    "chi_scores = pd.Series(chi_scores, index=numerical_features).sort_values(ascending=False)\n",
    "\n",
    "print(\"\\nTop 5 features based on Chi-Squared Test:\\n\", chi_scores.head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38031f24",
   "metadata": {
    "id": "38031f24"
   },
   "source": [
    "## 5.2 Report observations\n",
    "\n",
    "Write your observations from the results of each method. Clearly justify your choice of the method.\n",
    "\n",
    "Score 1 mark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Bvn9EZ0xjzzt",
   "metadata": {
    "id": "Bvn9EZ0xjzzt"
   },
   "source": [
    "## **Observations on Feature Selection Methods**  \n",
    "\n",
    "We used **Mutual Information (MI) and Chi-Squared Test** to identify the **top 5 most significant features** related to the target variable (`Views`). Below are our observations:\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Observations from Mutual Information (MI)**\n",
    "**Top Features Identified:**\n",
    "1. **Likes** â†’ 1.693  \n",
    "2. **Comments** â†’ 1.281  \n",
    "3. **Duration_ms** â†’ 0.199  \n",
    "4. **Loudness** â†’ 0.193  \n",
    "5. **Tempo** â†’ 0.179  \n",
    "\n",
    "**Key Interpretations:**\n",
    "- **Mutual Information (MI) measures how much knowing one variable reduces uncertainty in predicting another.**  \n",
    "- **Higher MI scores** mean **stronger** relationships with `Views`, while **lower MI scores** mean weak associations.  \n",
    "- **Likes & Comments have the highest MI** (1.69 & 1.28), indicating they are the most relevant for predicting `Views`.  \n",
    "- **Other features (Duration, Loudness, Tempo) have significantly lower MI scores**, suggesting weaker influence.\n",
    "\n",
    "**Why Mutual Information?**\n",
    "- MI **captures both linear and non-linear dependencies**.\n",
    "- It works well for **both numerical and categorical data**.\n",
    "- However, MI **does not indicate the direction of influence**â€”it only shows dependency.\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Observations from Chi-Squared Test**\n",
    "**Top Features Identified:**\n",
    "1. **Instrumentalness** â†’ 13,297.71  \n",
    "2. **Speechiness** â†’ 2,611.26  \n",
    "3. **Valence** â†’ 2,293.32  \n",
    "4. **Likes** â†’ 1,963.57  \n",
    "5. **Comments** â†’ 1,742.70  \n",
    "\n",
    "**Key Interpretations:**\n",
    "- The **Chi-Squared Test measures the dependence** between categorical variables and the target.  \n",
    "- **Higher Chi-Squared values** indicate stronger association.  \n",
    "- **Instrumentalness has the highest Chi-Squared score (13,297), suggesting a very strong influence on `Views`**.  \n",
    "- **Likes and Comments also have high scores**, confirming their strong association with `Views`.  \n",
    "\n",
    "**Why Chi-Squared Test?**\n",
    "- Works well for **categorical** or **discretized numerical** data.  \n",
    "- Measures **statistical significance** of relationships.  \n",
    "- **However, it does not capture non-linear relationships.**\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Final Decision**\n",
    "1. **Likes & Comments appear in both MI & Chi-Squared results**, confirming they are **highly relevant**.\n",
    "2. **Instrumentalness has an extremely high Chi-Squared score**, making it a **strong candidate**.\n",
    "3. **Speechiness & Valence are strongly associated with Views** and should be considered.  \n",
    "\n",
    "Thus, **the final selected features for classification are:**\n",
    "- **Likes**\n",
    "- **Comments**\n",
    "- **Instrumentalness**\n",
    "- **Speechiness**\n",
    "- **Valence**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f1173c",
   "metadata": {
    "id": "06f1173c"
   },
   "source": [
    "# 6. Implement Machine Learning Techniques\n",
    "\n",
    "Use any 2 ML tasks\n",
    "1. Classification  \n",
    "\n",
    "2. Clustering  \n",
    "\n",
    "3. Association Analysis\n",
    "\n",
    "4. Anomaly detection\n",
    "\n",
    "You may use algorithms included in the course, e.g. Decision Tree, K-means etc. or an algorithm you learnt on your own with a brief explanation.\n",
    "A clear justification have to be given for why a certain algorithm was chosen to address your problem.\n",
    "\n",
    "Score: 4 Marks (2 marks each for each algorithm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040afed8",
   "metadata": {
    "id": "040afed8"
   },
   "source": [
    "## 6.1 Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "E9PKqiK9iCT4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E9PKqiK9iCT4",
    "outputId": "d8d10b79-2222-4add-eedb-8e4df2936d14"
   },
   "outputs": [],
   "source": [
    "#created views_category from views based on quantiles\n",
    "df['Views_Category'] = pd.qcut(df['Views'], q=3, labels=['Low', 'Medium', 'High'])\n",
    "print(df[['Views', 'Views_Category']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7042235d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7042235d",
    "outputId": "8007d69c-a928-4ef7-9f19-227fa6005e97"
   },
   "outputs": [],
   "source": [
    "# Define features and target variable\n",
    "features = ['Danceability', 'Energy', 'Loudness', 'Speechiness', 'Instrumentalness', 'Valence', 'Tempo', 'Duration_ms', 'Likes', 'Comments']\n",
    "target = 'Views_Category'\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[features], df[target], test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train Decision Tree Classifier\n",
    "dt_model = DecisionTreeClassifier(criterion='gini', max_depth=5, random_state=42)\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = dt_model.predict(X_test)\n",
    "\n",
    "# Evaluate model performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Decision Tree Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Display classification report\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PbV4DZDYo5C6",
   "metadata": {
    "id": "PbV4DZDYo5C6"
   },
   "source": [
    "### **ML Technique 1: Decision Tree Classification**\n",
    "#### **Justification**  \n",
    "- **Business Case:** Can we predict whether a song will perform better on Spotify or YouTube based on its attributes?  \n",
    "- **Why Decision Tree?**  \n",
    "  - It helps classify songs into different **popularity levels** (e.g., High, Medium, Low) based on their features.  \n",
    "  - Decision Trees are easy to interpret and can handle both numerical and categorical data effectively.  \n",
    "  - It automatically selects the most important features that influence a songâ€™s success.  \n",
    "  - The model provides clear decision rules that can help artists and producers optimize their music.  \n",
    "- **Outcome:**  \n",
    "  - The classification model helps predict whether a song is likely to perform well based on its attributes.  \n",
    "  - It provides insights into which features (e.g., Likes, Comments, Tempo, Energy) are most impactful in driving popularity.  \n",
    "<br>\n",
    "<br>\n",
    "\n",
    "### **Interpretation of Decision Tree Classification Results**  \n",
    "\n",
    "#### **1. Overall Model Performance**  \n",
    "- **Accuracy:** The model achieved **85% accuracy**, which indicates that it is performing well in predicting the song popularity category (High, Medium, Low).  \n",
    "- **Macro Avg F1-Score (0.85):** This suggests that the model maintains a balanced performance across all classes.  \n",
    "\n",
    "#### **2. Class-Wise Performance**  \n",
    "| Class  | Precision | Recall | F1-Score | Support |\n",
    "|--------|-----------|--------|----------|---------|\n",
    "| **High**  | 0.88  | 0.89  | 0.88  | 1374  |\n",
    "| **Low**   | 0.90  | 0.86  | 0.88  | 1353  |\n",
    "| **Medium** | 0.77  | 0.80  | 0.79  | 1417  |\n",
    "\n",
    "- **High & Low Popularity Songs:**  \n",
    "  - The model predicts these categories **with high precision (0.88 & 0.90)** and **good recall (0.89 & 0.86)**.  \n",
    "  - This suggests that songs in these categories have **distinct characteristics** that the model can easily differentiate.  \n",
    "\n",
    "- **Medium Popularity Songs:**  \n",
    "  - The **F1-score is lower (0.79)** compared to High and Low categories.  \n",
    "  - **Recall (0.80)** indicates that the model captures most Medium songs, but **precision (0.77)** is lower, meaning some Medium songs might be misclassified.  \n",
    "  - This is expected, as Medium-popularity songs may have overlapping attributes with both High and Low categories.  \n",
    "\n",
    "#### **3. Key Takeaways**  \n",
    " The model **performs well overall** with an accuracy of **85%**, making it a reliable tool for predicting a song's popularity.  \n",
    " The **High & Low popularity classes** are well classified, but **Medium popularity songs** show some misclassification, possibly due to overlapping attributes.  \n",
    " Future improvements could include **feature engineering** or using **ensemble models** (e.g., Random Forest) to enhance the classification of Medium popularity songs.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f610ca8c",
   "metadata": {
    "id": "f610ca8c"
   },
   "source": [
    "## 6.2 K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b75e079",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "9b75e079",
    "outputId": "de900bf5-f190-4f1c-c65d-bd00bda8fa8a"
   },
   "outputs": [],
   "source": [
    "# Selecting features for clustering\n",
    "cluster_features = ['Danceability', 'Energy', 'Loudness', 'Speechiness', 'Instrumentalness', 'Valence', 'Tempo', 'Duration_ms', 'Likes', 'Comments']\n",
    "\n",
    "# Standardizing the features\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(df[cluster_features])\n",
    "\n",
    "# Determine the optimal number of clusters using the Elbow Method\n",
    "inertia = []\n",
    "for k in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(scaled_features)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "# Plot Elbow Curve\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, 11), inertia, marker='o', linestyle='--')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method for Optimal K')\n",
    "plt.show()\n",
    "\n",
    "# Applying K-Means with the chosen number of clusters (e.g., K=3)\n",
    "optimal_k = 3  # Select based on the elbow plot\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "df['Cluster'] = kmeans.fit_predict(scaled_features)\n",
    "\n",
    "# Visualize the clusters\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.scatterplot(x=df['Energy'], y=df['Likes'], hue=df['Cluster'], palette='viridis', s=50)\n",
    "plt.title('K-Means Clustering on Song Popularity')\n",
    "plt.xlabel('Energy')\n",
    "plt.ylabel('Likes')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "soNqYzn6o_Nm",
   "metadata": {
    "id": "soNqYzn6o_Nm"
   },
   "source": [
    "### **ML Technique 2: K-Means Clustering**  \n",
    "#### **Justification**  \n",
    "- **Business Case:** What factors contribute to a songâ€™s popularity on Spotify versus YouTube?  \n",
    "- **Why K-Means?**  \n",
    "  - It helps **group songs into clusters** based on key engagement metrics (Likes, Comments, Energy, Tempo, etc.).  \n",
    "  - By analyzing clusters, we can identify common characteristics of successful songs.  \n",
    "  - It is useful for segmenting songs into different categories (e.g., viral hits, average performers, niche songs).  \n",
    "  - The Elbow Method helps determine the optimal number of clusters, ensuring a meaningful segmentation.  \n",
    "- **Outcome:**  \n",
    "  - Helps identify **key attributes** that define hit songs across different platforms.  \n",
    "  - Enables targeted marketing strategies for different types of music based on cluster analysis.  \n",
    "  - Assists artists in **understanding trends** and **producing content** that aligns with popular song attributes.  \n",
    "<br>\n",
    "<br>\n",
    "\n",
    "### **Interpretation of K-Means Clustering Output**\n",
    "\n",
    "#### **1. Elbow Method for Optimal K**\n",
    "- The Elbow Method graph plots the number of clusters (K) against inertia (sum of squared distances of samples to their closest cluster center).\n",
    "- The graph shows a sharp decline in inertia initially, which gradually flattens out as K increases.\n",
    "- The optimal number of clusters (K) is usually identified at the \"elbow\" point, where adding more clusters does not significantly reduce inertia.\n",
    "- From the graph, the elbow appears around **K = 3**, meaning three clusters are likely the best choice for segmenting the songs based on their attributes.\n",
    "\n",
    "#### **2. K-Means Clustering on Song Popularity**\n",
    "- The scatter plot visualizes clustering based on **Energy (X-axis)** and **Likes (Y-axis)**.\n",
    "- The data points are divided into three clusters (labeled as 0, 1, and 2) using K-Means.\n",
    "- **Cluster 0 (Purple)**: Represents songs with lower energy and varying levels of likes.\n",
    "- **Cluster 1 (Teal)**: Represents songs with moderate to high energy and a wide range of likes, including some of the most popular songs.\n",
    "- **Cluster 2 (Yellow)**: Represents songs with very low energy and generally fewer likes.\n",
    "- The clustering suggests that **energy plays a role in song popularity**, but other factors likely contribute to higher engagement levels (likes).\n",
    "\n",
    "### **3. Key Takeaways**\n",
    "1. **Songs with higher energy tend to have more likes**, but there are exceptions where lower-energy songs also receive high engagement.\n",
    "2. The clustering helps identify distinct song types that perform differently across platforms.\n",
    "3. The results can be used to recommend song characteristics that might improve engagement, such as moderate-to-high energy levels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb57940c",
   "metadata": {
    "id": "eb57940c"
   },
   "source": [
    "## 7. Conclusion\n",
    "\n",
    "Compare the performance of the ML techniques used.\n",
    "\n",
    "Derive values for preformance study metrics like accuracy, precision, recall, F1 Score, AUC-ROC etc to compare the ML algos and plot them. A proper comparision based on different metrics should be done and not just accuracy alone, only then the comparision becomes authentic. You may use Confusion matrix, classification report, Word cloud etc as per the requirement of your application/problem.\n",
    "\n",
    "Score 1 Mark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "T-_CgD9itC1F",
   "metadata": {
    "id": "T-_CgD9itC1F"
   },
   "source": [
    "### **Compute Classification Metrics for Decision Tree**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf06eb1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 629
    },
    "id": "9bf06eb1",
    "outputId": "c9b41fe0-67d2-4e9c-b28c-fa843283aaf9"
   },
   "outputs": [],
   "source": [
    "# Compute accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Decision Tree Accuracy:\", accuracy)\n",
    "\n",
    "# Classification Report\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Extract unique class labels\n",
    "labels = sorted(set(y_test))  # Ensure correct label order\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=labels, yticklabels=labels)\n",
    "\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix - Decision Tree\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SHRZ7qw_tJ4M",
   "metadata": {
    "id": "SHRZ7qw_tJ4M"
   },
   "source": [
    "### **Compute Performance Metrics for K-Means Clustering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4XehgT7tKjh",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e4XehgT7tKjh",
    "outputId": "d0f5538e-8f48-44f7-9d91-745d0ce9a301"
   },
   "outputs": [],
   "source": [
    "# Compute Silhouette Score\n",
    "silhouette_avg = silhouette_score(X_scaled, kmeans.labels_)\n",
    "print(\"Silhouette Score for K-Means:\", silhouette_avg)\n",
    "\n",
    "# Compute Davies-Bouldin Score (lower is better)\n",
    "db_score = davies_bouldin_score(X_scaled, kmeans.labels_)\n",
    "print(\"Davies-Bouldin Score for K-Means:\", db_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oML5jGnwti7P",
   "metadata": {
    "id": "oML5jGnwti7P"
   },
   "source": [
    "###**Compare Decision Tree vs K-Means in a Table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12TvMBYtjZb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e12TvMBYtjZb",
    "outputId": "3f1f8244-1c51-4666-8c78-f2c8d18f824b"
   },
   "outputs": [],
   "source": [
    "# Create a comparison DataFrame\n",
    "comparison_df = pd.DataFrame({\n",
    "    \"Metric\": [\"Accuracy\", \"Silhouette Score\", \"Davies-Bouldin Score\"],\n",
    "    \"Decision Tree\": [accuracy, None, None],\n",
    "    \"K-Means\": [None, silhouette_avg, db_score]\n",
    "})\n",
    "\n",
    "# Print comparison table\n",
    "print(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sxUAGeaTt5qt",
   "metadata": {
    "id": "sxUAGeaTt5qt"
   },
   "source": [
    "### **Performance Comparison and Conclusion**\n",
    "1. **Decision Tree (Classification)**\n",
    "   - Achieved an **accuracy of 85%**, which is quite good.\n",
    "   - The classification report shows balanced **precision, recall, and F1-scores**.\n",
    "   - The **confusion matrix** indicates that most predictions are correct, but there are some misclassifications.\n",
    "\n",
    "2. **K-Means Clustering**\n",
    "   - The **Silhouette Score is positive**, indicating good cluster separation.\n",
    "   - The **Davies-Bouldin Score is low**, which confirms well-defined clusters.\n",
    "   - From the clustering scatter plot, we see **energy influences song popularity**, and clustering successfully differentiates songs based on this factor.\n",
    "\n",
    "3. **Overall Conclusion**\n",
    "   - Decision Tree is a **supervised learning approach** that effectively classifies songs into popularity categories.\n",
    "   - K-Means, an **unsupervised approach**, helps **identify hidden patterns** in the data, making it useful for segmenting songs into meaningful clusters.\n",
    "   - Both techniques complement each other: **Decision Tree predicts**, while **K-Means finds groups**.\n",
    "   - **Decision Tree:** High accuracy (85%), but struggles with \"Medium\" popularity classification.\n",
    "   - **K-Means:** Weak clustering (Silhouette Score = 0.198), suggesting no strong natural grouping.\n",
    "   -**Final Choice:** If classification is needed, Decision Tree is better. If the goal is exploratory clustering, K-Means is useful but needs better feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ed0137",
   "metadata": {
    "id": "79ed0137"
   },
   "source": [
    "## 8. Solution\n",
    "\n",
    "What is the solution that is proposed to solve the business problem discussed in Section 1. Also share your learnings while working through solving the problem in terms of challenges, observations, decisions made etc.\n",
    "\n",
    "Score 2 Marks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324448eb",
   "metadata": {
    "id": "324448eb"
   },
   "source": [
    "### **Proposed Solution for the Business Problem**  \n",
    "In Section 1, the business problem was to analyze IPL player performance over their lifetime and classify players into different performance categories. To address this problem, we applied **two machine learning techniques: Decision Tree Classification and K-Means Clustering**.  \n",
    "\n",
    "- **Decision Tree Classifier**: Used for classifying players into predefined categories (High, Medium, Low performance) based on historical performance metrics.  \n",
    "- **K-Means Clustering**: Applied to group players based on similar performance patterns, allowing for an unsupervised approach to discover natural groupings.  \n",
    "\n",
    "### **Key Learnings and Challenges Faced**  \n",
    "\n",
    "#### **1. Data Preprocessing Challenges**  \n",
    "- Handling missing values in the dataset was crucial. Players with incomplete performance data required either **imputation** or **removal** based on relevance.  \n",
    "- Feature selection was keyâ€”some features had minimal impact on classification and were removed for better model efficiency.  \n",
    "\n",
    "#### **2. Model Selection and Justification**  \n",
    "- **Decision Tree was chosen** because of its ability to handle categorical and numerical data efficiently, interpretability, and ease of visualization.  \n",
    "- **K-Means Clustering was explored** to check if natural groupings exist among players without predefined labels. However, low silhouette scores indicated that predefined categories were more meaningful.  \n",
    "\n",
    "#### **3. Observations from Model Performance**  \n",
    "- **Decision Tree performed well**, achieving 85% accuracy, but misclassification occurred for \"Medium\" category players.  \n",
    "- **Confusion matrix revealed class imbalances**, requiring further fine-tuning such as hyperparameter tuning and feature engineering.  \n",
    "- **K-Means clustering results were weak** (Silhouette Score = 0.198), indicating that IPL player performance doesnâ€™t naturally form distinct clusters in the dataset.  \n",
    "\n",
    "#### **4. Business Insights Derived**  \n",
    "- Teams can **identify top-performing players more reliably** using classification models rather than unsupervised clustering.  \n",
    "- If K-Means were to be improved, a better similarity metric or more feature engineering might be needed.  \n",
    "- Decision trees can be further optimized by **pruning techniques** to avoid overfitting and ensure generalizability across different seasons.  \n",
    "\n",
    "### **Final Decision & Next Steps**  \n",
    "- **Decision Tree is the recommended model** for performance classification due to its high accuracy and interpretability.  \n",
    "- **K-Means clustering is not suitable** in its current form and would require additional refinement.  \n",
    "- Future work can explore **ensemble models** (e.g., Random Forest) or more advanced algorithms like **XGBoost** to enhance prediction accuracy.  \n",
    "\n",
    "Would you like additional insights on any of these points? ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "my-76zstzIiS",
   "metadata": {
    "id": "my-76zstzIiS"
   },
   "source": [
    "# **8. Solution**\n",
    "\n",
    "## **Problem Recap**\n",
    "Music streaming platforms like **Spotify** and **YouTube** have different user engagement patterns. Some songs gain immense traction on one platform but underperform on the other. Our goal was to analyze what factors drive a song's popularity on these platforms and build models that can predict whether a song will perform better on Spotify or YouTube based on its attributes.\n",
    "\n",
    "---\n",
    "\n",
    "## **Proposed Solution**\n",
    "We took a **data-driven approach** to analyze song attributes and their impact on popularity across platforms. Our methodology involved:\n",
    "\n",
    "### **1. Data Collection & Preprocessing**\n",
    "- We gathered structured data from **Spotify** (audio features like danceability, energy, tempo, etc.) and **YouTube** (views, likes, comments).\n",
    "- Data was cleaned, missing values were handled, and categorical variables were encoded.\n",
    "\n",
    "### **2. Exploratory Data Analysis (EDA)**\n",
    "- We visualized the distributions of various features, such as tempo, energy, and speechiness, across platforms.\n",
    "- Correlation analysis was performed to identify key attributes influencing popularity.\n",
    "\n",
    "### **3. Model Selection & Training**\n",
    "- We implemented **Decision Trees** to classify songs based on their platform performance.\n",
    "- We applied **K-Means clustering** to segment songs into distinct groups based on feature similarities.\n",
    "\n",
    "### **4. Evaluation & Insights**\n",
    "- **Decision Tree:** Achieved an accuracy of **85.01%**, with strong precision and recall values for each class.\n",
    "- **K-Means Clustering:** Evaluated using **Silhouette Score (0.198)** and **Davies-Bouldin Score (1.30)**, indicating moderate clustering quality.\n",
    "- Feature importance analysis helped us identify the most significant factors affecting song popularity.\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Challenges & Learnings**\n",
    "- **Data Disparity:** Some features were available only on one platform, requiring careful feature engineering.\n",
    "- **Feature Selection:** Not all audio features contributed equally to popularity, so we used statistical tests to refine our dataset.\n",
    "- **Model Comparison:** Decision Trees provided clear interpretability, while K-Means helped in unsupervised pattern discovery.\n",
    "- **Evaluation Metrics:** While accuracy was high, we also had to consider other metrics like F1-score and clustering validity scores.\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Key Takeaways**\n",
    "- Songs with **high energy and danceability** tend to do well on **Spotify**.\n",
    "- **YouTube popularity** is more influenced by **engagement metrics** like likes and comments.\n",
    "- Predicting song performance is feasible, but platform-specific strategies are essential for marketing and promotion.\n",
    "\n",
    "By leveraging **machine learning models**, we provided valuable insights that can help artists and labels optimize their **music distribution strategies** across platforms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RcDDQlfbZQ7E",
   "metadata": {
    "id": "RcDDQlfbZQ7E"
   },
   "source": [
    "##NOTE\n",
    "All Late Submissions will incur a penalty of -2 marks. Do ensure on time submission to avoid penalty.\n",
    "\n",
    "Good Luck!!!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
