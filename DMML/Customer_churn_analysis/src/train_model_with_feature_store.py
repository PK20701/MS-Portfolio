import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from xgboost import XGBClassifier
import mlflow
from mlflow.tracking import MlflowClient
import logging
from pathlib import Path
from datetime import datetime
from typing import Dict, Any

# --- Project Root and Paths Configuration ---
project_root = Path(__file__).parent.parent
# The path to the final transformed data from the previous step
TRANSFORMED_DATA_PATH = project_root / "data" / "transformed" / "transformed_features.parquet"
MLFLOW_TRACKING_URI = (project_root / "mlruns").as_uri()
LOG_DIR = project_root / "logs"

# --- Logger Configuration ---
LOG_DIR.mkdir(parents=True, exist_ok=True)

# Create a logger for this specific module
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

# Avoid adding handlers if they already exist (e.g., from a previous import)
if not logger.handlers:
    log_file_path = LOG_DIR / f"train_model_{datetime.now().strftime('%Y-%m-%d')}.log"

    # Create handlers
    file_handler = logging.FileHandler(log_file_path)
    stream_handler = logging.StreamHandler()

    # Create formatter and add it to the handlers
    formatter = logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s")
    file_handler.setFormatter(formatter)
    stream_handler.setFormatter(formatter)

    # Add the handlers to the logger
    logger.addHandler(file_handler)
    logger.addHandler(stream_handler)

def get_training_data(path: Path) -> pd.DataFrame:
    """Loads the transformed training data from the specified parquet file."""
    logger.info(f"Loading training data from {path}...")
    if not path.exists():
        logger.error(f"Training data not found at {path}. Please run 'transform_and_store.py' first.")
        raise FileNotFoundError(f"Training data not found at {path}")
    
    df = pd.read_parquet(path)
    logger.info(f"Successfully loaded training data with shape: {df.shape}")
    return df

def evaluate_model(y_true: pd.Series, y_pred: pd.Series) -> Dict[str, float]:
    """Calculates and returns a dictionary of performance metrics."""
    # Use zero_division=0 to avoid warnings when a class is not predicted
    accuracy = accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred, zero_division=0)
    recall = recall_score(y_true, y_pred, zero_division=0)
    f1 = f1_score(y_true, y_pred, zero_division=0)
    return {"accuracy": accuracy, "precision": precision, "recall": recall, "f1_score": f1}

def train_and_log_models():
    """
    Trains multiple classification models, evaluates them, and logs results,
    parameters, and models to MLflow.
    """
    # --- 1. Get Data ---
    # This function retrieves data from the feature store and prepares it.
    logger.info("Retrieving training data from the local feature store (Parquet file)...")
    training_df = get_training_data(TRANSFORMED_DATA_PATH)

    # --- 2. Prepare Data for Modeling ---
    logger.info("--- Preparing data for modeling ---")
    
    # Define features (X) and target (y)
    TARGET_COL = 'Churn'
    # The other ID columns are not generated by the current pipeline.
    ID_COLS = ['customerID']
    
    if TARGET_COL not in training_df.columns:
        raise ValueError(f"Target column '{TARGET_COL}' not found in the training data.")

    y = training_df[TARGET_COL]
    # Programmatically find feature columns by excluding identifiers and the target.
    # Using a set for exclusion is slightly more efficient and robust.
    exclude_cols = set(ID_COLS + [TARGET_COL])
    feature_cols = [col for col in training_df.columns if col not in exclude_cols]
    X = training_df[feature_cols]
 
    # Split data
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
    logger.info(f"Training set size: {len(X_train)} samples")
    logger.info(f"Test set size: {len(X_test)} samples")

    # --- 3. Model Experimentation with MLflow ---
    logger.info("--- Starting Model Experimentation with MLflow ---")
    mlflow.set_experiment("CustomerChurnPrediction")

    # Define models to experiment with
    # We will add class_weight='balanced' to handle imbalanced data
    # and introduce XGBoost as a more powerful model.
    models = {
        "LogisticRegression": LogisticRegression(random_state=42, max_iter=2000, class_weight='balanced'),
        "RandomForest": RandomForestClassifier(random_state=42, class_weight='balanced'),
        "XGBoost": XGBClassifier(random_state=42, eval_metric='logloss')
    }

    # Define parameter grids for GridSearchCV to find the best hyperparameters
    param_grids = {
        "LogisticRegression": {
            'C': [0.1, 1, 10],
            'solver': ['liblinear']
        },
        "RandomForest": {
            'n_estimators': [100, 200],
            'max_depth': [10, 20],
            'min_samples_split': [2, 5]
        },
        "XGBoost": {
            'n_estimators': [100, 200],
            'max_depth': [3, 5],
            'learning_rate': [0.05, 0.1]
        }
    }

    # Prepare an input example for schema inference
    input_example = X_train.head(1)

    # Initialize MLflow client to interact with the model registry
    client = MlflowClient()

    for model_name, model in models.items():
        # Start a new MLflow run for each model tuning experiment
        with mlflow.start_run(run_name=f"{model_name}_HyperTuning"):
            logger.info(f"--- Tuning and logging model: {model_name} ---")
            
            # --- Hyperparameter Tuning with GridSearchCV ---
            # This will search for the best parameters using 5-fold cross-validation
            grid_search = GridSearchCV(
                estimator=model,
                param_grid=param_grids[model_name],
                cv=5,
                scoring='f1', # Optimize for F1-score, which is good for imbalanced classes
                n_jobs=-1 # Use all available CPU cores
            )
            grid_search.fit(X_train, y_train)

            best_model = grid_search.best_estimator_
            
            # Log best params and score from the search
            mlflow.log_params(grid_search.best_params_)
            mlflow.log_metric("best_cv_f1_score", grid_search.best_score_)
            logger.info(f"Best CV F1-score for {model_name}: {grid_search.best_score_:.4f}")
            
            y_pred = best_model.predict(X_test)
            metrics = evaluate_model(y_test, y_pred)
            mlflow.log_metrics(metrics)
            logger.info(f"Metrics for best {model_name} on test set: {metrics}")

            # --- Conditionally Register the Model ---
            # This logic robustly checks if a model should be registered.
            # It avoids relying on exception handling for this expected scenario.
            register_model = True
            f1_metric_key = "f1_score"
            new_f1_score = metrics.get(f1_metric_key, 0)

            # Use search_registered_models to safely check for model existence without raising an error.
            existing_model_versions = client.search_model_versions(f"name='{model_name}'")

            if not existing_model_versions:
                logger.info(f"No registered model named '{model_name}' found. Registering new model.")
                register_model = True
            else:
                # If model exists, find the best F1 score of all its versions to compare against.
                logger.info(f"Found existing registered model: '{model_name}'. Comparing F1 scores.")
                best_existing_f1 = 0
                for version in existing_model_versions:
                    run = client.get_run(version.run_id)
                    existing_f1 = run.data.metrics.get(f1_metric_key, 0)
                    if existing_f1 > best_existing_f1:
                        best_existing_f1 = existing_f1
                
                logger.info(f"New model F1 score: {new_f1_score:.4f}. Best existing F1 score: {best_existing_f1:.4f}")
                if new_f1_score <= best_existing_f1:
                    register_model = False
                    logger.info("New model is not an improvement. Not registering new version.")
            
            # Log the model artifact. Conditionally register it in the model registry.
            mlflow.sklearn.log_model(
                sk_model=best_model,
                artifact_path="model",
                registered_model_name=model_name if register_model else None,
                input_example=input_example
            )
            logger.info(f"Model '{model_name}' logged as an artifact. Registered: {register_model}")

    logger.info("--- Model experimentation finished. ---")
    logger.info("To view the results, run 'mlflow ui' in your terminal from the project root.")

def main():
    """Main execution function."""
    logger.info("--- Starting Model Training Script ---")
    try:
        # Set the MLflow tracking URI. By default, it logs to a local `mlruns` directory.
        mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)
        logger.info(f"MLflow tracking URI set to: {MLFLOW_TRACKING_URI}")
        
        train_and_log_models()
        
        logger.info("--- Model Training Script Finished Successfully ---")
    except Exception as e:
        logger.critical(f"An unexpected error occurred during the training process: {e}", exc_info=True)

if __name__ == "__main__":
    main()